{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code to test base models, make ensambles of several models and train 2-layer NN on top of base models predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "from scipy.misc import imread, imresize\n",
    "from keras.models import Model, load_model\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.layers.core import Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Flatten, Dense, merge\n",
    "import tensorflow as tf\n",
    "\n",
    "num_classes = 1054"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parallel(model, gpu_count):\n",
    "    def get_slice(data, idx, parts):\n",
    "        shape = tf.shape(data)\n",
    "        size = tf.concat([ shape[:1] // parts, shape[1:] ],axis=0)\n",
    "        stride = tf.concat([ shape[:1] // parts, shape[1:]*0 ],axis=0)\n",
    "        start = stride * idx\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    outputs_all = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        outputs_all.append([])\n",
    "\n",
    "    #Place a copy of the model on each GPU, each getting a slice of the batch\n",
    "    for i in range(gpu_count):\n",
    "        with tf.device('/gpu:%d' % i):\n",
    "            with tf.name_scope('tower_%d' % i) as scope:\n",
    "\n",
    "                inputs = []\n",
    "                #Slice each input into a piece for processing on this GPU\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_n = Lambda(get_slice, output_shape=input_shape, arguments={'idx':i,'parts':gpu_count})(x)\n",
    "                    inputs.append(slice_n)                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "                \n",
    "                #Save all the outputs for merging back together later\n",
    "                for l in range(len(outputs)):\n",
    "                    outputs_all[l].append(outputs[l])\n",
    "\n",
    "    # merge outputs on CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for outputs in outputs_all:\n",
    "            merged.append(merge(outputs, mode='concat', concat_axis=0))\n",
    "            \n",
    "        new_model = Model(input=model.inputs, output=merged)\n",
    "        ## to save initial model\n",
    "        funcType = type(model.save)\n",
    "        # monkeypatch the save to save just the underlying model\n",
    "        def new_save(self_,filepath, overwrite=True):\n",
    "            model.save(filepath, overwrite)\n",
    "        new_model.save=funcType(new_save, new_model)\n",
    "        return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_last_layer(base_model, nb_classes):\n",
    "    #add new layers\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x) #new global pooling layer layer\n",
    "    #x = Dense(1024, activation='relu')(x)\n",
    "    #x = Dropout(0.3)(x)\n",
    "    #x = Dense(512, activation='relu')(x)\n",
    "    #x = Dropout(0.3)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x) #new softmax layer\n",
    "    model = Model(input=base_model.input, output=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load pretrained models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    main_model = load_model('models/114.h5', custom_objects={'tf':tf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:1'):\n",
    "    additional_model = load_model('models/85.h5', custom_objects={'tf':tf})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test single and additional models combined on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(pic):\n",
    "    x = imread(io.BytesIO(pic['picture']))\n",
    "    x = imresize(x, (180, 180))\n",
    "    x = np.array(x, np.float32)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    img = preprocess_input(x)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "def indices():\n",
    "    folder = './data/files/train'\n",
    "    categories = [item[19:-1] for item in sorted(glob(\"./data/files/train/*/\"))]\n",
    "    indices2class = dict(zip(range(len(categories)), categories))\n",
    "    return indices2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm \n",
    "\n",
    "with open('./data/train.bson', 'rb') as fbson:\n",
    "    data = bson.decode_file_iter(fbson)\n",
    "    batch = np.empty((0,180,180,3))\n",
    "    ids = []\n",
    "    results = []\n",
    "    results_1 = []\n",
    "    results_2 = []\n",
    "    categories = []\n",
    "    #input_data = pd.DataFrame({'_id' : [], 'img' : []}, dtype = 'int32')\n",
    "    j = 0\n",
    "    indices2class = indices()\n",
    "    start = time.time()\n",
    "    for c, d in enumerate(data):\n",
    "        _id = d['_id']\n",
    "        category = d['category_id']\n",
    "        pics = d['imgs']\n",
    "        ids.append(_id)\n",
    "        categories.append(category)\n",
    "        for e, pic in enumerate(d['imgs']):\n",
    "            if e == 0:\n",
    "                img_0 = get_image(pic)\n",
    "            else:\n",
    "                batch = np.append(batch, get_image(pic), axis=0)\n",
    "        #predicting on model 2 - additional images\n",
    "        batch_size = batch.shape[0]\n",
    "        if batch_size != 0:\n",
    "            probabilities_2 = model2.predict(batch)\n",
    "            probabilities_sum_2 = np.sum(probabilities_2, axis = 0)/batch_size\n",
    "            prediction_2 = np.argmax(probabilities_sum_2)\n",
    "            predict_2 = int(indices2class[prediction_2])\n",
    "            results_2.append(predict_2)\n",
    "            batch = np.empty((0,180,180,3))\n",
    "        else:\n",
    "            results_2.append(0)\n",
    "\n",
    "        #predicting on model 1 - single image\n",
    "        probabilities_1 = model1.predict(img_0)\n",
    "        prediction_1 = np.argmax(probabilities_1)\n",
    "        predict_1 = int(indices2class[prediction_1])\n",
    "        results_1.append(predict_1)\n",
    "\n",
    "        #predicting on combined model 1&2            \n",
    "        if batch_size != 0:\n",
    "            probabilities = probabilities_1 + probabilities_sum_2\n",
    "        else:\n",
    "            probabilities = probabilities_1\n",
    "\n",
    "        prediction = np.argmax(probabilities)\n",
    "        predict = int(indices2class[prediction])\n",
    "        results.append(predict)\n",
    "\n",
    "        #j+= 1\n",
    "        #if j > 5000:\n",
    "    result = pd.DataFrame(np.column_stack((ids, results_1, results_2, results, categories)), dtype = 'int32')\n",
    "    result.columns = ['_id', 'predict_1', 'predict_2', 'predict', 'category_id']\n",
    "    result.to_csv(\"result.csv\", index = False)\n",
    "    print(\"saved\")\n",
    "    #print(result)\n",
    "    print(\"done\")\n",
    "    print(\"{} seconds passed\".format(start - time.time()))\n",
    "    j = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test models from bson file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing import image \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import multi_gpu_model\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import random, csv, bson, io, tqdm, time\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from scipy.misc import imread, imresize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "num_classes = 5270\n",
    "\n",
    "img_width = 180\n",
    "img_height = 180\n",
    "\n",
    "def make_category_tables():\n",
    "    cat2idx = {}\n",
    "    idx2cat = {}\n",
    "    for ir in categories_df.itertuples():\n",
    "        category_id = ir[0]\n",
    "        category_idx = ir[4]\n",
    "        cat2idx[category_id] = category_idx\n",
    "        idx2cat[category_idx] = category_id\n",
    "    return cat2idx, idx2cat\n",
    "\n",
    "categories_df = pd.read_csv(\"categories.csv\", index_col=0)\n",
    "cat2idx, idx2cat = make_category_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(x):\n",
    "    #x = np.expand_dims(x, axis=0)\n",
    "    img = preprocess_input(x)    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(x, n = 4):\n",
    "    output_size = (180,180)\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        x = imresize(x, (224, 224))\n",
    "        w, h, c = x.shape\n",
    "        th, tw = output_size\n",
    "        h_diff = h - th\n",
    "        w_diff = w - tw\n",
    "        i = random.randint(0, h_diff)\n",
    "        j = random.randint(0, w_diff)\n",
    "        x= x[j:j + tw, i:i + th,:]\n",
    "        x = np.array(x, np.float32)\n",
    "        x = preprocess_image(x)\n",
    "        result.append(x)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getId(probabilities):\n",
    "    prediction = np.argmax(probabilities)\n",
    "    predict = int(idx2cat[prediction])\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2indices():\n",
    "    idx2cat\n",
    "    indices2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2indices = {}\n",
    "for indice, clas in indices2class.items():\n",
    "    idx2indices[cat2idx[int(clas)]] = indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices2idxprob(probs):\n",
    "    new_probs = np.empty(probs.shape)\n",
    "    for idx, x in enumerate(probs):\n",
    "        new_probs[idx] = probs[idx2indices[idx]]\n",
    "    return new_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "import pdb\n",
    "\n",
    "n = 4\n",
    "\n",
    "with open('./data/train.bson', 'rb') as fbson:\n",
    "    data = bson.decode_file_iter(fbson)\n",
    "    batch = np.empty((0,180,180,3))\n",
    "    ids = []\n",
    "    results = []\n",
    "    main_batch = []\n",
    "    additional_batch = []\n",
    "    input_data = pd.DataFrame({'_id' : [], 'img' : []}, dtype = 'int32')\n",
    "    j = 0\n",
    "    indices2class = indices()\n",
    "    for c, d in tqdm(enumerate(data)):\n",
    "        if c < 5100:\n",
    "            continue\n",
    "        #start_time = time.time()\n",
    "        _id = d['_id']\n",
    "        category = d['category_id']\n",
    "        i = 0  \n",
    "        k = 0\n",
    "        for e, pic in enumerate(d['imgs']):\n",
    "            img = load_img(io.BytesIO(pic['picture']), target_size = (180, 180))\n",
    "            x = img_to_array(img)\n",
    "            #pdb.set_trace()\n",
    "            croped_imgs = crop_image(x, n)\n",
    "            main_batch.append(preprocess_image(x))\n",
    "            main_batch = main_batch + croped_imgs\n",
    "            if e != 0:\n",
    "                additional_batch.append(preprocess_image(x))\n",
    "                #additional_batch = additional_batch + croped_imgs\n",
    "                k += 1\n",
    "            i += 1\n",
    "        prob_main = main_model.predict(np.array(main_batch))\n",
    "        if (k > 0):\n",
    "            prob_additional = additional_model.predict(np.array(additional_batch))\n",
    "        print(\"\")\n",
    "        print(str(category) + \"actual\")\n",
    "        print(\"\")\n",
    "        prob_main_mean = np.mean(prob_main, axis = 0)\n",
    "        if (k > 0):\n",
    "            prob_additional_mean = np.mean(prob_additional, axis = 0)\n",
    "        #for i in range((e+1)*(n+1)):\n",
    "        #print(getId(prob_main_mean))\n",
    "        #prob_main_sum = np.sum(prob_main, axis = 0)\n",
    "        #probabilities_sum = np.sum(probabilities, axis = 0)  \n",
    "        #prediction = np.argmax(probabilities_sum)\n",
    "        #predict = int(indices2class[prediction])\n",
    "        #ids.append(_id)\n",
    "        #results.append(predict)\n",
    "        #second_time = time.time()\n",
    "        main_batch = []\n",
    "        additional_batch = []\n",
    "        k = 0\n",
    "        #j+= 1\n",
    "        #finish_time = time.time()\n",
    "        if c > 5150:\n",
    "            break\n",
    "    result = pd.DataFrame(np.column_stack((ids, results)), dtype = 'int32')\n",
    "    result.columns = ['_id', 'category_id']\n",
    "    result.to_csv(\"result.csv\", index = False)\n",
    "    print(\"saved\")\n",
    "    j = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test from batch one model with multicropping and avereging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_new(pic):\n",
    "    img = load_img(io.BytesIO(pic['picture']), target_size = (180, 180))\n",
    "    x = img_to_array(img)\n",
    "    img = preprocess_input(x)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_additional = model2\n",
    "model_single = model1\n",
    "batch_size = 256\n",
    "\n",
    "with open('./data/train.bson', 'rb') as fbson:\n",
    "    data = bson.decode_file_iter(fbson)\n",
    "    single = []\n",
    "    additional = []\n",
    "    additional_images = {}\n",
    "    additional_images[0] = 0\n",
    "    categories = []\n",
    "    batch = []\n",
    "    m = 0\n",
    "    n = 0 # product index in a batch\n",
    "    k = 0 # additional_images index in a batch\n",
    "    start_time = time.time()\n",
    "    for c, d in enumerate(data):  \n",
    "        _id = d['_id']\n",
    "        category = d['category_id']\n",
    "        pics = d['imgs']\n",
    "        categories.append(category)\n",
    "        \n",
    "        for e, pic in enumerate(d['imgs']):\n",
    "            if e == 0:\n",
    "                single.append(get_image_new(pic))\n",
    "            else:\n",
    "                additional.append(get_image_new(pic))\n",
    "                k += 1\n",
    "        #print(\"First --- %s seconds ---\" % (time.time() - start_time))\n",
    "        #start_time = time.time()\n",
    "        additional_images[n + 1] = k\n",
    "        n += 1\n",
    "        #print(\"Second --- %s seconds ---\" % (time.time() - start_time))\n",
    "        if n == batch_size*20:\n",
    "            result = predict(np.array(single), np.array(additional), additional_images, categories)\n",
    "            #encode Y\n",
    "            encoded_Y = encoder.transform(categories)\n",
    "            categories_one_hot = to_categorical(encoded_Y, num_classes = 5270)\n",
    "\n",
    "            k = 0\n",
    "            n = 0\n",
    "            additional_images = {}\n",
    "            additional_images[0] = 0\n",
    "            single = []\n",
    "            additional = []\n",
    "            result = []\n",
    "            categories = []\n",
    "\n",
    "        if c % 14000 == 0:\n",
    "            #print(\"Generating batch  --- %s seconds ---\" % (time.time() - start_time))\n",
    "            #start_time = time.time()\n",
    "            print(str(c) + \" products done. \" + str(c/7000000*100) + \"%%. --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(single_data, additional_data, additional_images, categories):\n",
    "    result = []\n",
    "    num_single = len(single_data)\n",
    "    num_additional = len(additional_data)\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "    )\n",
    "    \n",
    "    single_train_generator = datagen.flow(\n",
    "        single_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False,\n",
    "        )\n",
    "    \n",
    "    additional_train_generator = datagen.flow(\n",
    "        single_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False,\n",
    "        )\n",
    "\n",
    "    #print(\"Generating batch  --- %s seconds ---\" % (time.time() - start_time))\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    probabilities_additional = model_additional.predict_generator(additional_train_generator,\n",
    "                                                                   steps = num_additional//batch_size,\n",
    "                                                                   workers = 16)\n",
    "    #predicting on single model (first image)\n",
    "    probabilities_single = model_single.predict_generator(single_train_generator, \n",
    "                                                           steps = num_single//batch_size,\n",
    "                                                           workers = 16)\n",
    "    #print(\"Prediciting --- %s seconds ---\" % (time.time() - start_time))\n",
    "    #start_time = time.time()            \n",
    "\n",
    "    #compose resulting data frame\n",
    "    for i in range(n):\n",
    "        probabilities_additional_i_flatten = np.empty(0)\n",
    "        if additional_images[i + 1] - additional_images[i] != 0:\n",
    "            probabilities_additional_i_flatten = probabilities_additional[additional_images[i]:additional_images[i + 1],:].flatten()\n",
    "        probabilities = []\n",
    "\n",
    "        probabilities = probabilities_single[i,:].tolist()\n",
    "        if probabilities_additional_i_flatten.shape[0] != 0:\n",
    "            probabilities = probabilities + probabilities_additional_i_flatten.tolist()\n",
    "        # adding zeros at the end of every row if we have less than 4 photos\n",
    "        zeros = [0] * (4* num_classes - len(probabilities))\n",
    "        probabilities =  probabilities + zeros\n",
    "\n",
    "        assert len(probabilities) == 4* num_classes\n",
    "        #print(len(probabilities))\n",
    "        result.append(probabilities)\n",
    "    #print(\"Rearenging results --- %s seconds ---\" % (time.time() - start_time))\n",
    "    #start_time = time.time()\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
