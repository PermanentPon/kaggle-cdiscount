{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code to create, train and test 2 layers FC on top of features from base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing import image \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import multi_gpu_model\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import random, csv, bson, io, tqdm, time\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from scipy.misc import imread, imresize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "num_classes = 5270\n",
    "\n",
    "img_width = 180\n",
    "img_height = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10540, input_shape=(21080,), activation = 'relu'))\n",
    "    model.add(Dense(5270, activation = 'softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preparations and utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:1'):\n",
    "    model_single = load_model('models/74.hdf5', custom_objects={'tf':tf})\n",
    "\n",
    "with tf.device('/gpu:2'):\n",
    "    model_additional = load_model('models/85.h5', custom_objects={'tf':tf})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read categories and create decoder for one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/category_names.csv') \n",
    "categories_list = df.category_id\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(categories_list)\n",
    "encoded_Y = encoder.transform(categories_list)\n",
    "\n",
    "categories_one_hot = to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_new(pic):\n",
    "    img = load_img(io.BytesIO(pic['picture']), target_size = (180, 180))\n",
    "    x = img_to_array(img)\n",
    "    img = preprocess_input(x)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Getting base predictions and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(single_data, additional_data, additional_images, categories):\n",
    "    result = []\n",
    "    num_single = len(single_data)\n",
    "    num_additional = len(additional_data)\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "    )\n",
    "    \n",
    "    single_train_generator = datagen.flow(\n",
    "        single_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False,\n",
    "        )\n",
    "    \n",
    "    additional_train_generator = datagen.flow(\n",
    "        single_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False,\n",
    "        )\n",
    "\n",
    "    #print(\"Generating batch  --- %s seconds ---\" % (time.time() - start_time))\n",
    "    #start_time = time.time()\n",
    "    \n",
    "    probabilities_additional = model_additional.predict_generator(additional_train_generator,\n",
    "                                                                   steps = num_additional//batch_size,\n",
    "                                                                   workers = 16)\n",
    "    #predicting on single model (first image)\n",
    "    probabilities_single = model_single.predict_generator(single_train_generator, \n",
    "                                                           steps = num_single//batch_size,\n",
    "                                                           workers = 16)\n",
    "    #print(\"Prediciting --- %s seconds ---\" % (time.time() - start_time))\n",
    "    #start_time = time.time()            \n",
    "\n",
    "    #compose resulting data frame\n",
    "    for i in range(n):\n",
    "        probabilities_additional_i_flatten = np.empty(0)\n",
    "        if additional_images[i + 1] - additional_images[i] != 0:\n",
    "            probabilities_additional_i_flatten = probabilities_additional[additional_images[i]:additional_images[i + 1],:].flatten()\n",
    "        probabilities = []\n",
    "\n",
    "        probabilities = probabilities_single[i,:].tolist()\n",
    "        if probabilities_additional_i_flatten.shape[0] != 0:\n",
    "            probabilities = probabilities + probabilities_additional_i_flatten.tolist()\n",
    "        # adding zeros at the end of every row if we have less than 4 photos\n",
    "        zeros = [0] * (4* num_classes - len(probabilities))\n",
    "        probabilities =  probabilities + zeros\n",
    "\n",
    "        assert len(probabilities) == 4* num_classes\n",
    "        #print(len(probabilities))\n",
    "        result.append(probabilities)\n",
    "    #print(\"Rearenging results --- %s seconds ---\" % (time.time() - start_time))\n",
    "    #start_time = time.time()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./data/train.bson', 'rb') as fbson:\n",
    "    data = bson.decode_file_iter(fbson)\n",
    "    single = []\n",
    "    additional = []\n",
    "    additional_images = {}\n",
    "    additional_images[0] = 0\n",
    "    categories = []\n",
    "    batch = []\n",
    "    m = 0\n",
    "    n = 0 # product index in a batch\n",
    "    k = 0 # additional_images index in a batch\n",
    "    start_time = time.time()\n",
    "    for c, d in enumerate(data):  \n",
    "        _id = d['_id']\n",
    "        category = d['category_id']\n",
    "        pics = d['imgs']\n",
    "        categories.append(category)\n",
    "        \n",
    "        for e, pic in enumerate(d['imgs']):\n",
    "            if e == 0:\n",
    "                single.append(get_image_new(pic))\n",
    "            else:\n",
    "                additional.append(get_image_new(pic))\n",
    "                k += 1\n",
    "        #print(\"First --- %s seconds ---\" % (time.time() - start_time))\n",
    "        #start_time = time.time()\n",
    "        additional_images[n + 1] = k\n",
    "        n += 1\n",
    "        #print(\"Second --- %s seconds ---\" % (time.time() - start_time))\n",
    "        if n == batch_size*20:\n",
    "            result = predict(np.array(single), np.array(additional), additional_images, categories)\n",
    "            #encode Y\n",
    "            encoded_Y = encoder.transform(categories)\n",
    "            categories_one_hot = to_categorical(encoded_Y, num_classes = 5270)\n",
    "\n",
    "            #train 2nd layer model\n",
    "            #print(m)\n",
    "            #if m == 7:\n",
    "            #    #print (\"m == 0\")\n",
    "            #    print(model.test_on_batch(np.array(result), categories_one_hot))\n",
    "            #    m = 0\n",
    "            #model.train_on_batch(np.array(result), categories_one_hot)\n",
    "\n",
    "            #print(\"Model traing --- %s seconds ---\" % (time.time() - start_time))\n",
    "            #start_time = time.time()\n",
    "\n",
    "            k = 0\n",
    "            n = 0\n",
    "            additional_images = {}\n",
    "            additional_images[0] = 0\n",
    "            single = []\n",
    "            additional = []\n",
    "            result = []\n",
    "            categories = []\n",
    "\n",
    "        if c % 14000 == 0:\n",
    "            #print(\"Generating batch  --- %s seconds ---\" % (time.time() - start_time))\n",
    "            #start_time = time.time()\n",
    "            print(str(c) + \" products done. \" + str(c/7000000*100) + \"%%. --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/00_1410000.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/00_560000.h5', custom_objects={'tf':tf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./data/test.bson', 'rb') as fbson:\n",
    "    data = bson.decode_file_iter(fbson)\n",
    "    single = []\n",
    "    additional = []\n",
    "    \n",
    "    additional_images = {}\n",
    "    additional_images[0] = 0\n",
    "\n",
    "    categories = []\n",
    "    result = []\n",
    "    batch = []\n",
    "    ids = []\n",
    "    \n",
    "    final_predictions = []\n",
    "    \n",
    "    n = 0 # product index in a batch\n",
    "    k = 0 # additional_images index in a batch\n",
    "    start_time = time.time()\n",
    "    for c, d in enumerate(data):  \n",
    "        _id = d['_id']\n",
    "        pics = d['imgs']\n",
    "        ids.append(_id)\n",
    "        \n",
    "        for e, pic in enumerate(d['imgs']):\n",
    "            if e == 0:\n",
    "                single.append(get_image_new(pic))\n",
    "            else:\n",
    "                additional.append(get_image_new(pic))\n",
    "                k += 1\n",
    "        #print(\"First --- %s seconds ---\" % (time.time() - start_time))\n",
    "        #start_time = time.time()\n",
    "\n",
    "        #single.append(img_0)\n",
    "        #np.concatenate((additional, batch), axis = 0)\n",
    "        #batch = np.empty((0,180,180,3))\n",
    "\n",
    "        additional_images[n + 1] = k\n",
    "        n += 1\n",
    "        #print(\"Second --- %s seconds ---\" % (time.time() - start_time))\n",
    "          if n == batch_size*20:\n",
    "            result = predict(np.array(single), np.array(additional), additional_images, categories)\n",
    "            #encode Y\n",
    "            encoded_Y = encoder.transform(categories)\n",
    "            categories_one_hot = to_categorical(encoded_Y, num_classes = 5270)\n",
    "\n",
    "            #print(encoded_res)\n",
    "            final_predictions += encoder.inverse_transform(encoded_res).tolist()\n",
    "            \n",
    "            #print(\"Model traing --- %s seconds ---\" % (time.time() - start_time))\n",
    "            #start_time = time.time()\n",
    "            \n",
    "            k = 0\n",
    "            n = 0\n",
    "            additional_images = {}\n",
    "            additional_images[0] = 0\n",
    "            single = []\n",
    "            additional = []\n",
    "            result = []\n",
    "            categories = []\n",
    "\n",
    "        if c % 1768 == 0:\n",
    "            print(str(c) + \" products done. \" + str(c/1768182*100) + \"%%. --- %s seconds ---\" % (time.time() - start_time))\n",
    "if n != 0:\n",
    "    #start_time = time.time()\n",
    "    #predicting on additional model (using all images except for first one)\n",
    "    #print(\"additional_images\" + str(additional_images))\n",
    "    probabilities_additional = np.array(model_additional.predict(np.array(additional), batch_size = 100))\n",
    "    #predicting on single model (first image)\n",
    "    probabilities_single = np.array(model_single.predict(np.array(single), batch_size = 100))\n",
    "\n",
    "    #print(\"Prediciting --- %s seconds ---\" % (time.time() - start_time))\n",
    "    #start_time = time.time()            \n",
    "\n",
    "    #compose resulting data frame\n",
    "    for i in range(n):\n",
    "        probabilities_additional_i_flatten = np.empty(0)\n",
    "        #print(\"Single - \" + str(np.argmax(probabilities_single[i,:])))\n",
    "        if additional_images[i + 1] - additional_images[i] != 0:\n",
    "            #print(probabilities_additional[additional_images[i]:additional_images[i + 1],:])\n",
    "            #print(\"Additional - \" + str(np.argmax(probabilities_additional[additional_images[i],:])))\n",
    "            probabilities_additional_i_flatten = probabilities_additional[additional_images[i]:additional_images[i + 1],:].flatten()\n",
    "        probabilities = []\n",
    "\n",
    "        #print(\"Actual - \" + str(category))\n",
    "        #print(\"len probabilities_additional_i_flatten\" + str(len(probabilities_additional_i_flatten.tolist())))\n",
    "        probabilities = probabilities_single[i,:].tolist()\n",
    "        if probabilities_additional_i_flatten.shape[0] != 0:\n",
    "            probabilities = probabilities + probabilities_additional_i_flatten.tolist()\n",
    "        # adding zeros at the end of every row if we have less than 4 photos\n",
    "        zeros = [0] * (4* num_classes - len(probabilities))\n",
    "        probabilities =  probabilities + zeros\n",
    "\n",
    "        assert len(probabilities) == 4* num_classes\n",
    "        #print(len(probabilities))\n",
    "        result.append(probabilities)\n",
    "    #print(\"Rearenging results --- %s seconds ---\" % (time.time() - start_time))\n",
    "    #start_time = time.time()\n",
    "\n",
    "    prob = model.predict_on_batch(np.array(result))\n",
    "    encoded_res = np.argmax(prob, axis = 1)\n",
    "    #print(encoded_res)\n",
    "    final_predictions += encoder.inverse_transform(encoded_res).tolist()            \n",
    "\n",
    "to_save = pd.DataFrame(np.column_stack((ids, final_predictions)), dtype = 'int32')\n",
    "to_save.columns = ['_id', 'category_id']\n",
    "to_save.to_csv(\"result.csv\", index = False)\n",
    "print(\"saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
